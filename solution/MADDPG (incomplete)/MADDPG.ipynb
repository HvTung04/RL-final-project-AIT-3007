{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magent2.environments import battle_v4\n",
    "from MADDPG import MADDPG\n",
    "from ReplayBuffer import MultiAgentReplayBuffer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = battle_v4.parallel_env(map_size=45, max_cycles=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"n_agents\": env.team_sizes[0],\n",
    "    \"n_actions\": env.action_space(\"red_0\").n,\n",
    "    \"actor_dims\": 13*13*5,\n",
    "    \"critic_dims\": 45*45*5,\n",
    "    \"n_episodes\": 1000,\n",
    "    \"print_interval\": 10,\n",
    "    \"max_steps\": 300,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_maddpg = MADDPG(\n",
    "    actor_dims=config[\"actor_dims\"],\n",
    "    critic_dims=config[\"critic_dims\"],\n",
    "    n_agents=config[\"n_agents\"],\n",
    "    n_actions=config[\"n_actions\"],\n",
    "    handle=\"blue\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_maddpg = MADDPG(\n",
    "    actor_dims=config[\"actor_dims\"],\n",
    "    critic_dims=config[\"critic_dims\"],\n",
    "    n_agents=config[\"n_agents\"],\n",
    "    n_actions=config[\"n_actions\"],\n",
    "    handle=\"red\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_memory = MultiAgentReplayBuffer(\n",
    "    10000,\n",
    "    config[\"critic_dims\"],\n",
    "    config[\"actor_dims\"],\n",
    "    config[\"n_actions\"],\n",
    "    config[\"n_agents\"],\n",
    "    handle=\"blue\",\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_memory = MultiAgentReplayBuffer(\n",
    "    10000,\n",
    "    config[\"critic_dims\"],\n",
    "    config[\"actor_dims\"],\n",
    "    config[\"n_actions\"],\n",
    "    config[\"n_agents\"],\n",
    "    handle=\"red\",\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_score_history = []\n",
    "blue_best_score = -np.inf\n",
    "red_score_history = []\n",
    "red_best_score = -np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natsu\\Desktop\\Work\\RL-final-project-AIT-3007\\.venv\\lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Error detected in AddmmBackward0. Traceback of forward call that caused the error:\n",
      "  File \"C:\\Users\\natsu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\natsu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\natsu\\Desktop\\Work\\RL-final-project-AIT-3007\\.venv\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\natsu\\Desktop\\Work\\RL-final-project-AIT-3007\\.venv\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\natsu\\Desktop\\Work\\RL-final-project-AIT-3007\\.venv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\natsu\\Desktop\\Work\\RL-final-project-AIT-3007\\.venv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\natsu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\natsu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\natsu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\natsu\\Desktop\\Work\\RL-final-project-AIT-3007\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\natsu\\Desktop\\Work\\RL-final-project-AIT-3007\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\natsu\\Desktop\\Work\\RL-final-project-AIT-3007\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\natsu\\Desktop\\Work\\RL-final-project-AIT-3007\\.venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\natsu\\Desktop\\Work\\RL-final-project-AIT-3007\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\natsu\\Desktop\\Work\\RL-final-project-AIT-3007\\.venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\natsu\\Desktop\\Work\\RL-final-project-AIT-3007\\.venv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\natsu\\Desktop\\Work\\RL-final-project-AIT-3007\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\natsu\\Desktop\\Work\\RL-final-project-AIT-3007\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\natsu\\Desktop\\Work\\RL-final-project-AIT-3007\\.venv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\natsu\\Desktop\\Work\\RL-final-project-AIT-3007\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\natsu\\Desktop\\Work\\RL-final-project-AIT-3007\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\natsu\\Desktop\\Work\\RL-final-project-AIT-3007\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\natsu\\AppData\\Local\\Temp\\ipykernel_4224\\789476578.py\", line 26, in <module>\n",
      "    blue_maddpg.learn(blue_memory)\n",
      "  File \"c:\\Users\\natsu\\Desktop\\Work\\RL-final-project-AIT-3007\\MADDPG\\MADDPG.py\", line 54, in learn\n",
      "    new_pi = agent.target_actor.forward(new_states)\n",
      "  File \"c:\\Users\\natsu\\Desktop\\Work\\RL-final-project-AIT-3007\\MADDPG\\ActorNetwork.py\", line 22, in forward\n",
      "    pi = torch.softmax(self.pi(x), dim=-1)\n",
      "  File \"c:\\Users\\natsu\\Desktop\\Work\\RL-final-project-AIT-3007\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\natsu\\Desktop\\Work\\RL-final-project-AIT-3007\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\natsu\\Desktop\\Work\\RL-final-project-AIT-3007\\.venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 125, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      " (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\python_anomaly_mode.cpp:115.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [64, 21]], which is output 0 of AsStridedBackward0, is at version 3; expected version 2 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m red_memory\u001b[38;5;241m.\u001b[39mstore_transition(obs, state, red_actions, red_reward, obs_, next_state, red_done)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 26\u001b[0m     \u001b[43mblue_maddpg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblue_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     red_maddpg\u001b[38;5;241m.\u001b[39mlearn(red_memory)\n\u001b[0;32m     29\u001b[0m obs \u001b[38;5;241m=\u001b[39m obs_\n",
      "File \u001b[1;32mc:\\Users\\natsu\\Desktop\\Work\\RL-final-project-AIT-3007\\MADDPG\\MADDPG.py:76\u001b[0m, in \u001b[0;36mMADDPG.learn\u001b[1;34m(self, memory)\u001b[0m\n\u001b[0;32m     74\u001b[0m critic_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(target, critic_value)\n\u001b[0;32m     75\u001b[0m agent\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 76\u001b[0m \u001b[43mcritic_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m agent\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     79\u001b[0m actor_loss \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39mforward(states, mu)\u001b[38;5;241m.\u001b[39mflatten()\n",
      "File \u001b[1;32mc:\\Users\\natsu\\Desktop\\Work\\RL-final-project-AIT-3007\\.venv\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\natsu\\Desktop\\Work\\RL-final-project-AIT-3007\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\natsu\\Desktop\\Work\\RL-final-project-AIT-3007\\.venv\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [64, 21]], which is output 0 of AsStridedBackward0, is at version 3; expected version 2 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "for episode in range(config[\"n_episodes\"]):\n",
    "    obs = env.reset() # Dict {agent_name: obs}\n",
    "    state = env.state() # Initial state\n",
    "    blue_score = 0\n",
    "    red_score = 0\n",
    "\n",
    "    for step in range(config[\"max_steps\"]):\n",
    "        blue_actions = blue_maddpg.choose_action(obs)\n",
    "        red_actions = red_maddpg.choose_action(obs)\n",
    "        actions = {**blue_actions, **red_actions}\n",
    "\n",
    "        obs_, reward, termination, truncation, _ = env.step(actions)\n",
    "\n",
    "        blue_reward = [reward.get(agent_name, 0) for agent_name in blue_actions.keys()]\n",
    "        red_reward = [reward.get(agent_name, 0) for agent_name in red_actions.keys()]\n",
    "\n",
    "        blue_done = [termination.get(agent_name, True) or truncation.get(agent_name, True) for agent_name in blue_actions.keys()]\n",
    "        red_done = [termination.get(agent_name, True) or truncation.get(agent_name, True) for agent_name in red_actions.keys()]\n",
    "\n",
    "        next_state = env.state()\n",
    "\n",
    "        blue_memory.store_transition(obs, state, blue_actions, blue_reward, obs_, next_state, blue_done)\n",
    "        red_memory.store_transition(obs, state, red_actions, red_reward, obs_, next_state, red_done)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            blue_maddpg.learn(blue_memory)\n",
    "            red_maddpg.learn(red_memory)\n",
    "\n",
    "        obs = obs_\n",
    "        state = next_state\n",
    "\n",
    "        blue_score += sum(blue_reward)\n",
    "        red_score += sum(red_reward)\n",
    "\n",
    "        if all(blue_done) or all(red_done):\n",
    "            break\n",
    "    \n",
    "    blue_score_history.append(blue_score)\n",
    "    red_score_history.append(red_score)\n",
    "\n",
    "    blue_avg_score = np.mean(blue_score_history[-100:])\n",
    "    red_avg_score = np.mean(red_score_history[-100:])\n",
    "\n",
    "    if blue_avg_score > blue_best_score:\n",
    "        blue_best_score = blue_avg_score\n",
    "        blue_maddpg.save_checkpoint()\n",
    "    \n",
    "    if red_avg_score > red_best_score:\n",
    "        red_best_score = red_avg_score\n",
    "        red_maddpg.save_checkpoint()\n",
    "\n",
    "    print(f\"Episode: {episode}, Blue Score: {blue_score}, Red Score: {red_score}, Blue Avg Score: {blue_avg_score}, Red Avg Score: {red_avg_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlmagent2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
