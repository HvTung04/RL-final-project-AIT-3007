{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcQlw8v6b_sy",
        "outputId": "c8adde3c-9689-453e-bfd2-eabffaa7f544"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U git+https://github.com/Farama-Foundation/MAgent2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "VI0i44SXb1dg"
      },
      "outputs": [],
      "source": [
        "from magent2.environments import battle_v4\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Zt9DaFGZIAfY"
      },
      "outputs": [],
      "source": [
        "env = battle_v4.parallel_env(map_size=45,max_cycles=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "X2k2GAmyH1-Y"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"obs_shape\": env.observation_space(\"red_0\").shape,\n",
        "    \"action_dims\": int(env.action_space(\"red_0\").n),\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"epsilon\": 1.0,\n",
        "    \"epsilon_decay\": 0.998,\n",
        "    \"epsilon_min\": 0.05,\n",
        "    \"gamma\": 0.98, # discount\n",
        "    \"batch_size\": 512,\n",
        "    \"tau\": 0.005, # soft update,\n",
        "    \"red_update_interval\": 2,\n",
        "    \"blue_update_interval\": 2,\n",
        "    \"num_episode\": 150,\n",
        "    \"num_step\": 300,\n",
        "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "JeqEoh41jEc0"
      },
      "outputs": [],
      "source": [
        "class DuelingQNetwork(nn.Module):\n",
        "    def __init__(self, obs_shape, actions_dim):\n",
        "        super(DuelingQNetwork, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(obs_shape[-1], 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * obs_shape[0] * obs_shape[1], 128)\n",
        "\n",
        "        # Separate streams for value and advantage\n",
        "        self.value_stream = nn.Linear(128, 1)\n",
        "        self.advantage_stream = nn.Linear(128, actions_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 3, 1, 2)  # (batch_size, width, height, channels) -> (batch_size, channels, width, height)\n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "\n",
        "        x = x.reshape(x.size(0), -1)  # Flatten\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        value = self.value_stream(x)\n",
        "        advantage = self.advantage_stream(x)\n",
        "\n",
        "        # Combine value and advantage streams\n",
        "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
        "\n",
        "        return q_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llZZIwYBDCWh",
        "outputId": "c5d26bc0-cff5-4da2-adb7-5d9da0616b43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([10, 21])\n"
          ]
        }
      ],
      "source": [
        "def test_qnetwork():\n",
        "    obs_shape = (13, 13, 5)\n",
        "    actions_dim = 21\n",
        "    qnetwork = DuelingQNetwork(obs_shape, actions_dim)\n",
        "\n",
        "    test_input = torch.randn(10, *obs_shape)\n",
        "    output = qnetwork(test_input)\n",
        "    print(output.shape)\n",
        "\n",
        "test_qnetwork()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ZTrGCwarE1Mj"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "  def __init__(self, buffer_size):\n",
        "    self.buffer_size = buffer_size\n",
        "    self.buffer = deque(maxlen=buffer_size)\n",
        "\n",
        "  def add(self, state, action, reward, next_state, done):\n",
        "    self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "  def sample(self, batch_size, device):\n",
        "    samples = random.sample(self.buffer, batch_size)\n",
        "\n",
        "    states, actions, rewards, next_states, dones = zip(*samples)\n",
        "\n",
        "    return (\n",
        "        torch.cat(states).to(device),\n",
        "        torch.tensor(actions, dtype=torch.long).unsqueeze(1).to(device),\n",
        "        torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(device),\n",
        "        torch.cat(next_states).to(device),\n",
        "        torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "    )\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qfkXziUlJXdA"
      },
      "outputs": [],
      "source": [
        "def update_network(network, target_network):\n",
        "  target_network.load_state_dict(network.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_eWRONHHrMK",
        "outputId": "512b4d6c-05c8-4bd4-e2e6-302b4f76897b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DuelingQNetwork(\n",
              "  (conv1): Conv2d(5, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (fc1): Linear(in_features=10816, out_features=128, bias=True)\n",
              "  (value_stream): Linear(in_features=128, out_features=1, bias=True)\n",
              "  (advantage_stream): Linear(in_features=128, out_features=21, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "red_q_network = DuelingQNetwork(\n",
        "    obs_shape=config[\"obs_shape\"],\n",
        "    actions_dim=config[\"action_dims\"]\n",
        ").to(config[\"device\"])\n",
        "\n",
        "red_target_q_network = DuelingQNetwork(\n",
        "    obs_shape=config[\"obs_shape\"],\n",
        "    actions_dim=config[\"action_dims\"]\n",
        ").to(config[\"device\"])\n",
        "\n",
        "red_optimizer = optim.Adam(red_q_network.parameters(), lr=config[\"learning_rate\"])\n",
        "red_buffer = ReplayBuffer(buffer_size=100000)\n",
        "\n",
        "update_network(red_q_network, red_target_q_network)\n",
        "red_target_q_network.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heDhJSpSJQF-",
        "outputId": "52404e32-1aad-4bc9-dd7c-5f0e2e466c5d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DuelingQNetwork(\n",
              "  (conv1): Conv2d(5, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (fc1): Linear(in_features=10816, out_features=128, bias=True)\n",
              "  (value_stream): Linear(in_features=128, out_features=1, bias=True)\n",
              "  (advantage_stream): Linear(in_features=128, out_features=21, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "blue_q_network = DuelingQNetwork(\n",
        "    obs_shape=config[\"obs_shape\"],\n",
        "    actions_dim=config[\"action_dims\"]\n",
        ").to(config[\"device\"])\n",
        "\n",
        "blue_target_q_network = DuelingQNetwork(\n",
        "    obs_shape=config[\"obs_shape\"],\n",
        "    actions_dim=config[\"action_dims\"]\n",
        ").to(config[\"device\"])\n",
        "\n",
        "blue_optimizer = optim.Adam(blue_q_network.parameters(), lr=config[\"learning_rate\"])\n",
        "blue_buffer = ReplayBuffer(buffer_size=100000)\n",
        "\n",
        "update_network(blue_q_network, blue_target_q_network)\n",
        "blue_target_q_network.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eBrs93SJ02L",
        "outputId": "b1156b4c-3de4-48fb-bbb1-74d25d611ae7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 26%|██▌       | 39/150 [09:12<25:32, 13.81s/it, Red Reward=-725, Blue Reward=-738, Epsilon=0.927]"
          ]
        }
      ],
      "source": [
        "pbar = tqdm(range(config[\"num_episode\"]))\n",
        "for episode in pbar:\n",
        "  obs = env.reset()[0]  # This return a tuple on colab, dict on local\n",
        "  red_total_reward = 0\n",
        "  blue_total_reward = 0\n",
        "  red_losses = []\n",
        "  blue_losses = []\n",
        "\n",
        "  done_agents = set()\n",
        "\n",
        "  for step in range(config[\"num_step\"]):\n",
        "    actions = {}\n",
        "\n",
        "    red_team = [agent for agent in env.agents if \"red\" in agent and agent not in done_agents]\n",
        "    blue_team = [agent for agent in env.agents if \"blue\" in agent and agent not in done_agents]\n",
        "\n",
        "    if len(red_team) > 0:\n",
        "      red_team_state = torch.stack(\n",
        "          [torch.tensor(obs[agent], dtype=torch.float32) for agent in red_team]\n",
        "      ).to(config[\"device\"])\n",
        "\n",
        "      with torch.no_grad():\n",
        "        red_q_values = red_q_network(red_team_state)\n",
        "        network_actions = torch.argmax(red_q_values, dim=1)\n",
        "\n",
        "      random_actions = torch.randint(0, config[\"action_dims\"], (len(red_team),), device=config[\"device\"])\n",
        "\n",
        "      red_actions = torch.where(\n",
        "          torch.rand(len(red_team), device=config[\"device\"]) < config[\"epsilon\"],\n",
        "          random_actions,\n",
        "          network_actions\n",
        "      ).to(config[\"device\"])\n",
        "\n",
        "      actions.update({agent: action.item() for agent, action in zip(red_team, red_actions)})\n",
        "\n",
        "    if len(blue_team) > 0:\n",
        "      blue_team_state = torch.stack(\n",
        "          [torch.tensor(obs[agent], dtype=torch.float32) for agent in blue_team]\n",
        "      ).to(config[\"device\"])\n",
        "\n",
        "      with torch.no_grad():\n",
        "        blue_q_values = blue_q_network(blue_team_state)\n",
        "        network_actions = torch.argmax(blue_q_values, dim=1)\n",
        "\n",
        "      random_actions = torch.randint(0, config[\"action_dims\"], (len(blue_team),), device=config[\"device\"])\n",
        "\n",
        "      blue_actions = torch.where(\n",
        "          torch.rand(len(blue_team), device=config[\"device\"]) < config[\"epsilon\"],\n",
        "          random_actions,\n",
        "          network_actions\n",
        "      ).to(config[\"device\"])\n",
        "\n",
        "      actions.update({agent: action.item() for agent, action in zip(blue_team, blue_actions)})\n",
        "\n",
        "    next_obs, rewards, terminations, truncations, infos = env.step(actions)\n",
        "    dones = {agent: terminations.get(agent, False) or truncations.get(agent, False) for agent in env.agents}\n",
        "\n",
        "    for agent in red_team:\n",
        "      if agent in done_agents:\n",
        "        continue\n",
        "      state = torch.tensor(obs[agent], dtype=torch.float32).unsqueeze(0).to(config[\"device\"])\n",
        "      action = actions[agent]\n",
        "      reward = rewards.get(agent, 0.0)\n",
        "      next_state = torch.tensor(next_obs[agent], dtype=torch.float32).unsqueeze(0).to(config[\"device\"])\n",
        "      done = dones.get(agent, False)\n",
        "\n",
        "      red_buffer.add(state, action, reward, next_state, done)\n",
        "      red_total_reward += reward\n",
        "\n",
        "      if done:\n",
        "        done_agents.add(agent)\n",
        "\n",
        "    for agent in blue_team:\n",
        "      if agent in done_agents:\n",
        "        continue\n",
        "      state = torch.tensor(obs[agent], dtype=torch.float32).unsqueeze(0).to(config[\"device\"])\n",
        "      action = actions[agent]\n",
        "      reward = rewards.get(agent, 0.0)\n",
        "      next_state = torch.tensor(next_obs[agent], dtype=torch.float32).unsqueeze(0).to(config[\"device\"])\n",
        "      done = dones.get(agent, False)\n",
        "\n",
        "      blue_buffer.add(state, action, reward, next_state, done)\n",
        "      blue_total_reward += reward\n",
        "\n",
        "      if done:\n",
        "        done_agents.add(agent)\n",
        "\n",
        "    obs = next_obs\n",
        "\n",
        "    if len(red_buffer) >= config[\"batch_size\"]:\n",
        "      states, actions, rewards, next_states, dones = red_buffer.sample(config[\"batch_size\"], config[\"device\"])\n",
        "\n",
        "      q_values = red_q_network(states).gather(1, actions)\n",
        "      with torch.no_grad():\n",
        "        next_actions = red_q_network(next_states).argmax(1, keepdim=True)\n",
        "        next_q_values = red_target_q_network(next_states).gather(1, next_actions)\n",
        "        target_q_values = rewards + (1 - dones) * config[\"gamma\"] * next_q_values\n",
        "\n",
        "      loss = F.mse_loss(q_values, target_q_values)\n",
        "      red_losses.append(loss.item())\n",
        "      red_optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      red_optimizer.step()\n",
        "\n",
        "      for target_param, local_param in zip(red_target_q_network.parameters(), red_q_network.parameters()):\n",
        "        target_param.data.copy_(config[\"tau\"] * local_param.data + (1.0 - config[\"tau\"]) * target_param.data)\n",
        "\n",
        "    if len(blue_buffer) >= config[\"batch_size\"]:\n",
        "      states, actions, rewards, next_states, dones = blue_buffer.sample(config[\"batch_size\"], config[\"device\"])\n",
        "\n",
        "      q_values = blue_q_network(states).gather(1, actions)\n",
        "      with torch.no_grad():\n",
        "        next_actions = blue_q_network(next_states).argmax(1, keepdim=True)\n",
        "        next_q_values = blue_target_q_network(next_states).gather(1, next_actions)\n",
        "        target_q_values = rewards + (1 - dones) * config[\"gamma\"] * next_q_values\n",
        "\n",
        "      loss = F.mse_loss(q_values, target_q_values)\n",
        "      blue_losses.append(loss.item())\n",
        "      blue_optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      blue_optimizer.step()\n",
        "\n",
        "      for target_param, local_param in zip(blue_target_q_network.parameters(), blue_q_network.parameters()):\n",
        "        target_param.data.copy_(config[\"tau\"] * local_param.data + (1.0 - config[\"tau\"]) * target_param.data)\n",
        "\n",
        "  pbar.set_postfix({\n",
        "      'Red Reward': red_total_reward,\n",
        "      'Blue Reward': blue_total_reward,\n",
        "      'Epsilon': config['epsilon']\n",
        "  })\n",
        "  if config[\"epsilon\"] > config[\"epsilon_min\"]:\n",
        "    config[\"epsilon\"] *= config[\"epsilon_decay\"]\n",
        "\n",
        "  if episode % config[\"red_update_interval\"] == 0:\n",
        "    update_network(red_q_network, red_target_q_network)\n",
        "  if episode % config[\"blue_update_interval\"] == 0:\n",
        "    update_network(blue_q_network, blue_target_q_network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxQmQHt2a0pw"
      },
      "outputs": [],
      "source": [
        "torch.save(blue_q_network.state_dict(), \"blue_dueling_q_default.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3J8NoU78QwKn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.save(\"red_losses.npy\", np.array(red_losses))\n",
        "np.save(\"blue_losses.npy\", np.array(blue_losses))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
